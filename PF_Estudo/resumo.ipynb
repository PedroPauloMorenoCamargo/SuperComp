{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paralelismo Teoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiste no uso de múltiplos processadores, simultaneamente, para resolver um problema.  \n",
    "Tem por objetivo o aumento do desempenho, ou seja, a redução do tempo necessário para resolver um problema.  \n",
    "\n",
    "Usamos paralelismo normalmente por dois motivos:  \n",
    "\n",
    "1. **Problemas cada vez mais complexos e/ou maiores**  \n",
    "2. **Clock dos processadores se aproximando dos limites ditados pela física**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomia de Flynn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Taxonomia de Flynn é uma forma de classificar computadores paralelos.  Proposta por **Flynn, em 1972**, baseia-se no fato de que um computador executa uma sequência de instruções sobre uma sequência de dados.  \n",
    "\n",
    "![Taxonomia de Flynn](img/Image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sistemas MultiCore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![Taxonomia de Flynn2](img/Image.png)\n",
    "### ![Sistemas Multicore](img/Image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMP (Symmetric Multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexto Histórico\n",
    "Não faz muito tempo que todos os PCs continham um único processador de propósito geral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avanço Tecnológico\n",
    "- **Demanda crescente por desempenho**: Com o aumento da necessidade de maior capacidade de processamento, e a redução dos custos dos processadores, os fabricantes passaram a introduzir sistemas com uma organização SMP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição de SMP\n",
    "O termo **SMP** se refere a:\n",
    "1. **Arquitetura de hardware computacional**: Vários processadores trabalhando em um único sistema de memória compartilhada.\n",
    "2. **Comportamento do sistema operacional**: Reflete a arquitetura SMP, possibilitando a utilização eficiente de múltiplos processadores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é Dependência?\n",
    "- Uma **dependência** ocorre quando uma iteração depende de resultados calculados em iterações anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paralelizabilidade\n",
    "- Quando **não existe nenhuma dependência** em um loop, dizemos que ele é **ingenuamente paralelizável**.  \n",
    "Isso significa que as iterações podem ser executadas simultaneamente, sem a necessidade de sincronização entre elas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de Paralelismo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paralelismo de Dados\n",
    "- A **mesma operação (lenta)** é executada para todos os elementos de um **conjunto de dados (grande)**.\n",
    "- Exemplos:\n",
    "  - Aplicação de filtros em imagens.\n",
    "  - Operações matemáticas sobre matrizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paralelismo de Tarefas\n",
    "- **Duas ou mais tarefas independentes** são executadas em paralelo.\n",
    "- Quando há dependências:\n",
    "  - O problema é **quebrado em partes independentes**.\n",
    "  - As partes são executadas na **ordem adequada** para evitar conflitos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/image3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paralelismo Prática (OPENMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sintaxe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/Image5.png)\n",
    "\n",
    "![Taxonomia de Flynn](img/Image6.png)\n",
    "\n",
    "![Taxonomia de Flynn](img/Image7.png)\n",
    "\n",
    "![Taxonomia de Flynn](img/Image8.png)\n",
    "\n",
    "![Taxonomia de Flynn](img/Image9.png)\n",
    "\n",
    "![Taxonomia de Flynn](img/Image10.png)\n",
    "\n",
    "![Taxonomia de Flynn](img/Image11.png)\n",
    "\n",
    "![Taxonomia de Flynn](img/Image13.png)\n",
    "\n",
    "![Taxonomia de Flynn](img/Image14.png)\n",
    "\n",
    "![Taxonomia de Flynn](img/Image15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilar OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/Image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é?\n",
    "A diretiva **`sections`** divide o trabalho de forma **não iterativa** em seções separadas, onde cada seção será executada por uma **thread** do grupo.  \n",
    "- Representa a implementação de **paralelismo funcional** (ou seja, paralelismo por código)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observações Importantes\n",
    "\n",
    "1. **Definição de Seções**:  \n",
    "   - A diretiva **`sections`** define a parte do código sequencial onde as **seções independentes** serão especificadas, utilizando a diretiva **`section`**.\n",
    "\n",
    "2. **Execução das Seções**:  \n",
    "   - Cada **`section`** será executada por uma **thread** do grupo de threads.\n",
    "\n",
    "3. **Sincronização Implícita**:  \n",
    "   - Existe um **ponto de sincronização implícita** no final da diretiva **`sections`**, a menos que se especifique o atributo **`nowait`**.\n",
    "\n",
    "4. **Alocação de Threads**:  \n",
    "   - Caso existam **mais threads do que seções**, o OpenMP decidirá:\n",
    "     - Quais threads executarão os blocos de **`section`**.\n",
    "     - Quais threads ficarão inativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é uma Tarefa?\n",
    "- Uma **tarefa** é definida em um **bloco estruturado de código**.\n",
    "- Representa uma **unidade de trabalho independente**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características das Tarefas\n",
    "\n",
    "1. **Aninhamento de Tarefas**:  \n",
    "   - As tarefas podem ser **aninhadas**, ou seja, uma tarefa pode **gerar outras novas tarefas**.\n",
    "\n",
    "2. **Alocação de Threads**:  \n",
    "   - Cada **thread** pode ser alocada para executar uma **tarefa**.\n",
    "\n",
    "3. **Execução Não Ordenada**:  \n",
    "   - Não há uma ordenação específica para o início das tarefas, elas são iniciadas conforme disponibilidade.\n",
    "\n",
    "4. **Independência**:  \n",
    "   - As tarefas são projetadas para serem **independentes**, permitindo maior flexibilidade e paralelismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/Image12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visão Geral\n",
    "O **método de escalonamento (`schedule`)** em OpenMP define como as iterações de um loop são atribuídas às threads. Aqui está uma explicação detalhada dos métodos **static**, **dynamic** e **guided**, com exemplos práticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo\n",
    "Loop com 16 iterações (`for (int i = 0; i < 16; i++)`) e 4 threads:\n",
    "\n",
    "- **`schedule(static)`** (padrão):\n",
    "  - Iterações são divididas igualmente:\n",
    "    - Thread 0: `{0, 1, 2, 3}`\n",
    "    - Thread 1: `{4, 5, 6, 7}`\n",
    "    - Thread 2: `{8, 9, 10, 11}`\n",
    "    - Thread 3: `{12, 13, 14, 15}`\n",
    "\n",
    "- **`schedule(static, 2)`**:\n",
    "  - Iterações divididas em blocos de 2:\n",
    "    - Thread 0: `{0, 1, 8, 9}`\n",
    "    - Thread 1: `{2, 3, 10, 11}`\n",
    "    - Thread 2: `{4, 5, 12, 13}`\n",
    "    - Thread 3: `{6, 7, 14, 15}`\n",
    "\n",
    "### Quando usar\n",
    "- **Workloads previsíveis e uniformes**, como processamento de imagens:\n",
    "```cpp\n",
    "#pragma omp parallel for schedule(static)\n",
    "for (int i = 0; i < num_rows; i++) {\n",
    "    process_row(image[i]); // Cada linha leva o mesmo tempo\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Dynamic Scheduling**\n",
    "- **Como funciona**:\n",
    "  - As threads solicitam blocos de iterações dinamicamente durante a execução.\n",
    "  - O tamanho do bloco é definido por `chunk_size`. O padrão é `chunk_size = 1`.\n",
    "  - Ideal para workloads imprevisíveis ou não uniformes.\n",
    "\n",
    "#### Exemplo\n",
    "Loop com 16 iterações, 4 threads e `schedule(dynamic, 2)`:\n",
    "- Blocos são distribuídos dinamicamente:\n",
    "  - Thread 0: `{0, 1}`\n",
    "  - Thread 1: `{2, 3}`\n",
    "  - Threads que terminarem primeiro recebem o próximo bloco:\n",
    "    - Ex.: Thread 0 -> `{4, 5}`, Thread 1 -> `{6, 7}`, e assim por diante.\n",
    "\n",
    "#### Quando usar\n",
    "- **Workloads imprevisíveis ou irregulares**, como busca em listas de tarefas:\n",
    "```cpp\n",
    "#pragma omp parallel for schedule(dynamic, 2)\n",
    "for (int i = 0; i < num_tasks; i++) {\n",
    "    process_task(tasks[i]); // Algumas tarefas levam mais tempo\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Guided Scheduling\n",
    "\n",
    "**Como funciona:**\n",
    "- As threads recebem blocos de iterações dinamicamente, mas o tamanho dos blocos diminui exponencialmente.\n",
    "- Começa com blocos grandes e reduz progressivamente até o tamanho mínimo especificado por `chunk_size`.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Loop com 16 iterações, 4 threads e `schedule(guided, 2)`:\n",
    "\n",
    "- **Blocos grandes no início:**\n",
    "  - Thread 0: `{0, 1, 2, 3, 4, 5}`\n",
    "  - Thread 1: `{6, 7, 8, 9}`\n",
    "- **Blocos menores conforme o trabalho avança:**\n",
    "  - Thread 0: `{10, 11}`\n",
    "  - Thread 1: `{12, 13}`\n",
    "- **Pequenos blocos finais:**\n",
    "  - Thread 0: `{14}`\n",
    "  - Thread 1: `{15}`\n",
    "\n",
    "**Quando usar:**\n",
    "\n",
    "Workloads com tempo decrescente ao longo do loop, como multiplicação de matrizes esparsas:\n",
    "\n",
    "```c\n",
    "#pragma omp parallel for schedule(guided, 4)\n",
    "for (int i = 0; i < num_rows; i++) {\n",
    "    multiply_sparse_row(matrix[i], vector); // Linhas maiores processadas primeiro\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação dos Métodos\n",
    "\n",
    "| **Método**  | **Divisão das Iterações**                                | **Quando Usar**                                        |\n",
    "|-------------|---------------------------------------------------------|-------------------------------------------------------|\n",
    "| **Static**  | Divisão fixa em blocos determinados no início.           | Workloads previsíveis e uniformes.                   |\n",
    "| **Dynamic** | Blocos distribuídos dinamicamente conforme threads terminam. | Workloads imprevisíveis ou irregulares.              |\n",
    "| **Guided**  | Blocos grandes no início, diminuindo exponencialmente.   | Workloads com tempo decrescente ao longo do loop.    |\n",
    "| **Auto**    | O sistema decide o método ideal.                         | Cenários experimentais ou genéricos.                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração de Números Aleatórios em Ambientes Paralelos\n",
    "\n",
    "A geração de números aleatórios em ambientes paralelos é complicada porque é necessário garantir que cada thread tenha números **independentes**, evitando que compartilhem o mesmo estado, o que pode causar:\n",
    "\n",
    "- **Conflitos nos resultados**, diminuindo a precisão e a acurácia do algoritmo.\n",
    "- **Impactos no desempenho**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solução com Sincronização\n",
    "\n",
    "Uma solução possível é **sincronizar o acesso** ao gerador de números aleatórios, utilizando áreas críticas. Contudo, isso apresenta uma desvantagem significativa:\n",
    "\n",
    "> O código se torna quase inteiramente **sequencial**, levando a perdas de desempenho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solução com RNG para cada thread\n",
    "\n",
    "A falta de acurácia e precisão, nesse caso, não vem do acesso concorrente. O principal problema é que cada thread utiliza um gerador de números aleatórios com “seeds” diferentes, o que pode introduzir:\n",
    "\n",
    "- **Viés entre as distribuições**: as sequências geradas por cada thread podem não ser **independentes** ou **bem distribuídas** quando combinadas.\n",
    "\n",
    "Esses desafios destacam a complexidade de implementar soluções robustas para geração de números aleatórios em sistemas paralel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPI: Message Passing Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paralelismo de tarefas, em memória distribuída"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Características Principais\n",
    "\n",
    "- **Assume que não há compartilhamento de memória** entre os processos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsabilidades do Programador\n",
    "\n",
    "1. **Dividir os dados**:\n",
    "   - Garantir que cada processo receba apenas os dados necessários para realizar sua parte do trabalho.\n",
    "\n",
    "2. **Trabalhar com interações bilaterais**:\n",
    "   - Utilizar operações como `send` e `receive` para a comunicação entre processos.\n",
    "\n",
    "3. **Reduzir a comunicação (quando possível)**:\n",
    "   - Minimizar a quantidade de dados transferidos entre os processos para otimizar o desempenho geral do programa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/Image16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceitos do MPI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank\n",
    "- Cada processo possui uma única **identificação (rank)**, atribuída automaticamente pelo sistema ao iniciar.\n",
    "- Essa identificação é:\n",
    "  - **Contínua**: começa em `0` e vai até `n-1` processos.\n",
    "  - Usada para diferenciar e endereçar os processos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group\n",
    "- Um **grupo** é um conjunto ordenado de `N` processos.\n",
    "- Cada grupo está associado a um **communicator**.\n",
    "- Inicialmente, todos os processos pertencem a um grupo padrão chamado **MPI_COMM_WORLD**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communicator\n",
    "- O **communicator** define:\n",
    "  - Uma **coleção de processos** (grupo).\n",
    "  - O **contexto** no qual os processos podem se comunicar.\n",
    "- O MPI utiliza a combinação de **grupo** e **contexto** para:\n",
    "  - Garantir uma comunicação segura.\n",
    "  - Prevenir problemas no envio de mensagens entre os processos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/Image17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sintaxe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/Image18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/Image19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/Image20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/Image21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI_Send(&mensagem, 1, MPI_INT, rank + 1, 0, MPI_COMM_WORLD);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** O endereço da variável a ser enviada, nesse caso **mensagem**.\n",
    "- **2.** A quantidade de elementos a serem enviados de certo tipo, nesse caso **1**.\n",
    "- **3.** O tipo a ser enviado, nesse caso **MPI_INT**\n",
    "- **4.** O rank de destino, nesse caso **rank + 1**\n",
    "- **5.** A tag da mensagem, nesse caso **0**\n",
    "- **6.** O comunicador que define o grupo de processos, nesse caso **MPI_COMM_WORLD** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI_Recv(&mensagem, 1, MPI_INT, size - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** O endereço da variável a ser recebida, nesse caso **mensagem**.\n",
    "- **2.** A quantidade de elementos a serem recebidos de certo tipo, nesse caso **1**.\n",
    "- **3.** O tipo a ser recebido, nesse caso **MPI_INT**\n",
    "- **4.** O rank de envio, nesse caso **size -1**\n",
    "- **5.** A tag da mensagem, nesse caso **0**\n",
    "- **6.** O comunicador que define o grupo de processos, nesse caso **MPI_COMM_WORLD** \n",
    "- **7.** Status da mensagem, nesse caso ignoramos ele **MPI_STATUS_IGNORE** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast e Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mandar uma mensagem para todos os processos: MPI_Bcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPI_Bcast(&numIterations, 1, MPI_INT, 0, MPI_COMM_WORLD);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** O endereço da variável a ser propagada, nesse caso **numIterations**.\n",
    "- **2.** A quantidade de elementos a serem propagados de certo tipo, nesse caso **1**.\n",
    "- **3.** O tipo a ser propagado, nesse caso **MPI_INT**\n",
    "- **4.** O rank do processo que irá realizar a propagação, nesse caso **0**\n",
    "- **5.** O comunicador que define o grupo de processos, nesse caso **MPI_COMM_WORLD** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** O endereço das variáveis dos processos a serem reduzidas, nesse caso **local_sum**.\n",
    "- **2.** Variável onde será guardada a redução, nesse caso **global_sum**.\n",
    "- **3.** A quantidade de elementos a serem reduzidos, nesse caso **1**.\n",
    "- **4.** O tipo a ser propagado, nesse caso **MPI_INT**\n",
    "- **4.** A função de redução, nesse caso **MPI_SUM**\n",
    "- **5.** O rank do processo que irá realizar a propagação, nesse caso **0**\n",
    "- **6.** O comunicador que define o grupo de processos, nesse caso **MPI_COMM_WORLD** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/Image22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter/Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root manda uma quantidade igual de dados para todos os outros processos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPI_Scatter(data.data(), localSize, MPI_INT, localData.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** Representa o buffer de envio contendo os dados a serem distribuídos, nesse caso **data**.\n",
    "- **2.** Número de elementos a serem enviados para cada processo, nesse caso **localSize = data.size()/n_processos**.\n",
    "- **3.** Tipo de dado do buffer de envio, nesse caso **MPI_INT**\n",
    "- **4.** Buffer de recepção local em cada processo, nesse caso **localData.data()**\n",
    "- **5.** Número de elementos que cada processo espera receber, nesse caso **localSize**\n",
    "- **6.** Tipo de dado do buffer de recepção, nesse caso **MPI_INT**\n",
    "- **7.** Rank do processo root, aquele que contém o array original a ser distribuído., nesse caso **0**\n",
    "- **8.** O comunicador que inclui todos os processos participantes da operação, nesse caso **MPI_COMM_WORLD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPI_Scatterv(data.data(), send_counts.data(), displs.data(), MPI_INT, local_data.data(), local_chunk_size, MPI_INT, 0, MPI_COMM_WORLD);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** Representa o buffer de envio contendo os dados a serem distribuídos, nesse caso **data**.\n",
    "- **2.** Número de elementos a serem enviados para cada processo, nesse caso **send_counts.data()**.\n",
    "- **3.** Index de Início da chunk de cada processo, nesse caso **displs.data()**.\n",
    "- **4.** Tipo de dado do buffer de envio, nesse caso **MPI_INT**\n",
    "- **5.** Buffer de recepção local em cada processo, nesse caso **localData.data()**\n",
    "- **6.** Número de elementos que cada processo espera receber, nesse caso **local_chunk_size**\n",
    "- **7.** Tipo de dado do buffer de recepção, nesse caso **MPI_INT**\n",
    "- **8.** Rank do processo root, aquele que contém o array original a ser distribuído., nesse caso **0**\n",
    "- **9.** O comunicador que inclui todos os processos participantes da operação, nesse caso **MPI_COMM_WORLD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPI_Gather(&localAverage, 1, MPI_DOUBLE, localAverages.data(), 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** Endereço do dado a ser enviado por cada processo, nesse caso **localAverage**.\n",
    "- **2.** Número de elementos a serem enviados para cada processo, nesse caso **1**.\n",
    "- **3.** Tipo de dado do buffer de envio, nesse caso **MPI_DOUBLE**\n",
    "- **4.** Ponteiro para o buffer onde os dados coletados serão armazenados no processo root, nesse caso **localAverages.data()**\n",
    "- **5.** Número de elementos esperados de cada processo, nesse caso **1**\n",
    "- **6.** Tipo de dado esperado de cada processo, nesse caso **MPI_DOUBLE**\n",
    "- **7.** Rank do processo root,onde os dados coletados serão armazenados, nesse caso **0**\n",
    "- **8.** O comunicador que inclui todos os processos participantes da operação, nesse caso **MPI_COMM_WORLD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/Image24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomia de Flynn](img/Image23.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
